# -*- coding: utf-8 -*-
"""GroqResearchBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w17b6FA1eYRH9CsM8Zqp7zZ6Zhm8Unyg
"""

!pip install -q groq gradio pypdf

# =========================
# STEP 1: INSTALL PACKAGES
# =========================
import subprocess
import sys

print("Installing required packages...")
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "groq", "gradio", "pypdf"])
print("Packages installed!")

# =========================================================
# STEP 2: IMPORTS
# =========================================================
import gradio as gr
from groq import Groq
from pypdf import PdfReader

print("All imports successful!")

# =========================================================
# CUSTOM CSS for Times New Roman + Sticky Tabs
# =========================================================
CUSTOM_CSS = """
* {
    font-family: "Times New Roman", serif !important;
}
.tabs, .gradio-tabs {
    position: sticky !important;
    top: 0;
    z-index: 999;
    background: white;
}
"""

# =========================================================
# SMART SUMMARY CLASSIFIER
# =========================================================
def is_research_text(text, assistant):
    """Classify whether the text is research-like."""
    prompt = f"""
    Decide if the following text is a research article.

    Text:
    {text[:2000]}

    Answer only YES or NO.
    """

    response = assistant.client.chat.completions.create(
        model=assistant.model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=3,
        temperature=0
    )

    ans = response.choices[0].message.content.strip().upper()
    return "YES" in ans

# =========================================================
# RESEARCH ASSISTANT CLASS
# =========================================================
class GroqResearchAssistant:
    def __init__(self, api_key: str):
        self.client = Groq(api_key=api_key)
        self.model = "llama-3.3-70b-versatile"

    def extract_text_from_pdf(self, path: str):
        if path is None:
            return ""
        try:
            reader = PdfReader(path)
            texts = [page.extract_text() or "" for page in reader.pages]
            return "\n".join(texts)
        except Exception as e:
            return f"Error reading PDF: {str(e)}"

    # ---------------------------------------
    # SMART SUMMARY: Research OR General
    # ---------------------------------------
    def smart_summarize(self, text, focus="general"):

        if is_research_text(text, self):
            # STRUCTURED RESEARCH SUMMARY
            prompt = f"""
            Provide a structured research summary of the following text.

            Focus on: {focus}

            Text:
            {text[:8000]}

            Include:
            1. Main Research Question or Objective
            2. Methodology
            3. Key Findings
            4. Implications or Conclusions
            """
        else:
            # SIMPLE GENERAL SUMMARY
            prompt = f"""
            Provide a clear, simple summary of the following text.

            Text:
            {text[:8000]}

            Do NOT assume it is research. Just summarize naturally.
            """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are an expert summarizer."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=600,
            temperature=0.4
        )
        return response.choices[0].message.content

    # ---------------------------------------
    # GENERAL ANSWERING FOR Q&A (WITH REFERENCES)
    # ---------------------------------------
    def answer_query(self, query, context="", history=None):
        if history is None:
            history = []

        messages = [
            {
                "role": "system",
                "content": (
                    "You are an expert research assistant. "
                    "Provide accurate, structured academic answers. "
                    "At the end of every answer, include a 'References:' section "
                    "with 2‚Äì4 credible academic sources (textbooks, papers, "
                    "or authoritative websites). "
                    "Do NOT fabricate DOIs or page numbers. "
                    "If unsure, provide general but valid references."
                )
            }
        ]

        for user_msg, bot_msg in history[-10:]:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": bot_msg})

        if context:
            query = f"Context:\n{context}\n\nQuestion: {query}"

        messages.append({"role": "user", "content": query})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=900,
            temperature=0.4
        )
        return response.choices[0].message.content

    # ---------------------------------------
    # LITERATURE RECOMMENDER
    # ---------------------------------------
    def recommend_literature(self, topic: str, num: int = 5):
        prompt = f"""
        Recommend {num} important research papers or books on: {topic}

        For each item include:
        - Title
        - Authors
        - Why it is relevant
        """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are an expert research librarian."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=900,
            temperature=0.8
        )
        return response.choices[0].message.content

    # ---------------------------------------
    # CODE GENERATOR
    # ---------------------------------------
    def generate_code(self, instruction: str, language: str = "python"):
        prompt = f"""
        You are an expert programmer.

        Task:
        {instruction}

        Target language: {language}

        Respond with code only (inside a code block).
        """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You generate high-quality code."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.3
        )
        return response.choices[0].message.content


# =========================================================
# MULTI-TURN TEXT Q&A CHAT
# =========================================================
text_chat_history = []

def text_chat_fn(question, text):
    global text_chat_history
    if assistant is None:
        return text_chat_history + [("System", "‚ö†Ô∏è Initialize assistant first.")]
    if not text.strip():
        return text_chat_history + [("System", "‚ö†Ô∏è Paste text first.")]
    ans = assistant.answer_query(question, context=text[:8000], history=text_chat_history)
    text_chat_history.append((question, ans))
    return text_chat_history

def text_chat_wrapper(question, text):
    history = text_chat_fn(question, text)
    return history, ""  # clear input box

def reset_text_chat():
    global text_chat_history
    text_chat_history = []
    return []


# =========================================================
# MULTI-TURN PDF Q&A CHAT
# =========================================================
pdf_chat_history = []

def pdf_chat_fn(question, path):
    global pdf_chat_history
    if assistant is None:
        return pdf_chat_history + [("System", "‚ö†Ô∏è Initialize first.")]
    if path is None:
        return pdf_chat_history + [("System", "‚ö†Ô∏è Upload a PDF first.")]
    txt = assistant.extract_text_from_pdf(path)
    if txt.startswith("Error"):
        return pdf_chat_history + [("System", txt)]
    ans = assistant.answer_query(question, context=txt[:8000], history=pdf_chat_history)
    pdf_chat_history.append((question, ans))
    return pdf_chat_history

def pdf_chat_wrapper(question, path):
    history = pdf_chat_fn(question, path)
    return history, ""  # clear input box

def reset_pdf_chat():
    global pdf_chat_history
    pdf_chat_history = []
    return []


# =========================================================
# SINGLE-SHOT PDF Q&A
# =========================================================
def custom_pdf_qa_fn(question, path):
    if assistant is None:
        return "‚ö†Ô∏è Initialize assistant first."
    if path is None:
        return "‚ö†Ô∏è Upload a PDF first."

    txt = assistant.extract_text_from_pdf(path)
    if txt.startswith("Error"):
        return txt

    return assistant.answer_query(question, context=txt[:8000])


# =========================================================
# GRADIO UI
# =========================================================
assistant = None

def create_interface():
    with gr.Blocks(title="Research Assistant", css=CUSTOM_CSS, theme=gr.themes.Soft()) as demo:

        gr.Markdown("# üéì Research Assistant Chatbot")
        gr.Markdown("### AI-Powered Research Helper")

        # API KEY
        with gr.Row():
            with gr.Column():
                gr.Markdown("### üîë Enter Your Groq API Key")
                api_key_box = gr.Textbox(type="password")
                start_button = gr.Button("Initialize Assistant", variant="primary")
                status_message = gr.Markdown("")

        def init_assistant(api_key):
            global assistant
            if not api_key.strip():
                return "‚ùå Provide a valid key!"
            try:
                assistant = GroqResearchAssistant(api_key)
                return "‚úÖ Assistant Ready!"
            except Exception as e:
                return f"‚ùå Error: {str(e)}"

        start_button.click(init_assistant, api_key_box, status_message)

        gr.Markdown("---")

        # ================================
        # TAB 1: GENERAL ASK (WITH REFERENCES)
        # ================================
        with gr.Tab("üí¨ Ask Questions"):
            chat = gr.Chatbot(type="tuples", height=400)
            qbox = gr.Textbox(label="Your Question")
            ask_btn = gr.Button("Ask")

            def ask_fn(question, history):
                if assistant is None:
                    return history + [("System", "‚ö†Ô∏è Initialize assistant first.")], ""
                answer = assistant.answer_query(question, history=history)
                history.append((question, answer))
                return history, ""

            ask_btn.click(ask_fn, [qbox, chat], [chat, qbox])
            qbox.submit(ask_fn, [qbox, chat], [chat, qbox])
            gr.Button("Clear Chat").click(lambda: [], None, chat)

        # ================================
        # TAB 2: TEXT SUMMARY + MULTI-TURN CHAT
        # ================================
        with gr.Tab("üìÑ Summarize Text"):
            text_input = gr.Textbox(lines=12, label="Paste Text Here")
            focus = gr.Dropdown(
                ["general", "methodology", "results", "implications", "limitations"],
                value="general", label="Summary Focus")
            sum_btn = gr.Button("Summarize")
            sum_out = gr.Textbox(lines=10, label="Summary Output")

            sum_btn.click(lambda t, f: assistant.smart_summarize(t, f), [text_input, focus], sum_out)

            gr.Markdown("### üí¨ Text Q&A (multi-turn)")
            text_chatbox = gr.Chatbot(type="tuples", height=350)
            text_q = gr.Textbox(label="Ask a question")
            text_ask_btn = gr.Button("Send")
            text_reset = gr.Button("Reset Chat")

            text_ask_btn.click(text_chat_wrapper, [text_q, text_input], [text_chatbox, text_q])
            text_q.submit(text_chat_wrapper, [text_q, text_input], [text_chatbox, text_q])
            text_reset.click(reset_text_chat, None, text_chatbox)

        # ================================
        # TAB 3: PDF ASSISTANT (MULTI-TURN)
        # ================================
        with gr.Tab("üìÇ PDF Assistant"):
            pdf_file = gr.File(label="Upload PDF")
            pdf_focus = gr.Dropdown(
                ["general", "methodology", "results", "implications", "limitations"],
                value="general", label="Summary Focus")
            pdf_sum_btn = gr.Button("Summarize PDF")
            pdf_sum_out = gr.Textbox(lines=12, label="PDF Summary")

            def summarize_pdf(f, foc):
                txt = assistant.extract_text_from_pdf(f)
                return assistant.smart_summarize(txt, foc)

            pdf_sum_btn.click(summarize_pdf, [pdf_file, pdf_focus], pdf_sum_out)

            gr.Markdown("### üí¨ Chat about this PDF (multi-turn)")
            pdf_chatbox = gr.Chatbot(type="tuples", height=300)
            pdf_q = gr.Textbox(label="Ask a PDF Question")
            pdf_ask_btn = gr.Button("Send")
            pdf_reset = gr.Button("Reset PDF Chat")

            pdf_ask_btn.click(pdf_chat_wrapper, [pdf_q, pdf_file], [pdf_chatbox, pdf_q])
            pdf_q.submit(pdf_chat_wrapper, [pdf_q, pdf_file], [pdf_chatbox, pdf_q])
            pdf_reset.click(reset_pdf_chat, None, pdf_chatbox)

        # ================================
        # TAB 4: LITERATURE
        # ================================
        with gr.Tab("üìö Literature Recommendations"):
            topic = gr.Textbox(label="Topic")
            num = gr.Slider(3, 10, value=5)
            lit_btn = gr.Button("Get Recommendations")
            lit_out = gr.Textbox(lines=12)

            lit_btn.click(lambda t, n: assistant.recommend_literature(t, n), [topic, num], lit_out)

        # ================================
        # TAB 5: CODE GENERATOR
        # ================================
        with gr.Tab("üíª Code Generator"):
            gr.Markdown("Generate code from natural language instructions.")
            code_lang = gr.Dropdown(
                ["python", "c", "cpp", "java", "javascript", "sql"],
                value="python", label="Language"
            )
            code_prompt = gr.Textbox(lines=4, label="Describe the code you want")
            code_btn = gr.Button("Generate Code")
            code_out = gr.Textbox(lines=16, label="Generated Code")

            def code_gen_fn(instr, lang):
                if assistant is None:
                    return "‚ö†Ô∏è Initialize assistant first."
                return assistant.generate_code(instr, lang)

            code_btn.click(code_gen_fn, [code_prompt, code_lang], code_out)

        # ================================
        # TAB 6: INFO
        # ================================
        with gr.Tab("‚ÑπÔ∏è Info"):
            gr.Markdown("""
            ## About This Research Assistant

            - Smart summarizer (research-aware)
            - Chat over uploaded text with memory
            - Chat over PDFs with multi-turn reasoning
            - Get literature recommendations
            - Generate code in multiple languages
            - Times New Roman academic interface
            - Sticky tabs for easier navigation

            **Model:** Llama 3.3 70B (Groq)
            """)

    return demo


# =========================================================
# LAUNCH
# =========================================================
print("üöÄ Launching...")
demo = create_interface()
demo.launch(share=True, debug=True)
print("App running!")